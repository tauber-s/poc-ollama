{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576bb321-1e43-409f-ac45-f8d3beea04d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "# import json\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_CLIENT_URL = f\"{OLLAMA_BASE_URL}/v1\"\n",
    "OLLAMA_API_URL = f\"{OLLAMA_BASE_URL}/api\"\n",
    "\n",
    "# Configure OpenAI client to use Ollama\n",
    "# Ollama doesn't require a real API key, but OpenAI client expects one\n",
    "client = openai.OpenAI(\n",
    "    base_url=OLLAMA_CLIENT_URL,\n",
    "    api_key=\"ollama\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789d5b56-0710-4587-8e67-830a70d5c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(OLLAMA_BASE_URL)\n",
    "        if response.status_code == 200:\n",
    "            return True, \"SUCCESS: Ollama is running!\"\n",
    "        else:\n",
    "            return False, f\"ERROR: Ollama responded with status code: {response.status_code}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False, f\"ERROR: Cannot connect to Ollama: {e}\"\n",
    "\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"List all available models\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API_URL}/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            return [model['name'] for model in models]\n",
    "        return []\n",
    "    except requests.exceptions.RequestException:\n",
    "        return []\n",
    "\n",
    "\n",
    "def simple_chat_example(model_name: str):\n",
    "    \"\"\"Basic chat completion example\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d177672-3811-46f3-b744-11d5261afd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Ollama connection...\n",
      "Status: SUCCESS: Ollama is running!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Ollama connection...\")\n",
    "is_running, status_message = check_ollama_status()\n",
    "print(f\"Status: {status_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f154397-ff49-4727-bf8d-f6ae90b4136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['deepseek-r1:latest']\n"
     ]
    }
   ],
   "source": [
    "if is_running:\n",
    "    available_models = list_available_models()\n",
    "    if not available_models:\n",
    "        print(\"No models found!\")\n",
    "    print(f\"Available models: {available_models}\")\n",
    "else:\n",
    "    print(\"Ollama not running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c718515f-3f51-4588-aaf6-57f29bc4b6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: deepseek-r1:latest\n",
      "\n",
      "Response:\n",
      "<think>\n",
      "Okay, the user asked me to explain quantum computing in simple terms, let's see what do that. They probably not expert but want basic understanding not too deep dive just simple analogy or complex explanation. Maybe compare bits like classical vs quantum computing. Use light analogy with light switch. Quantum superposition means particle being both 0and1and1like at once. Entanglement spooky action at distance connection Not sure. Decoherence. Need keep simple. Use everyday analogies. Light switch for superposition think dimmer switch classicalonoffonoff. For qubit bothbrighton can brightnotoffdim. Entangle. Entangle like two lightbulbs same switch same state. Deco like exposure air leak sunlight. Need structure. Make sure accurate but simple. Okay. Start with analogy. Alright let think. Classical computer bits bit is light switch onoff or off Quantum qubit dimmer range. Then superposition both at once. Entangle. Decoherence\n"
     ]
    }
   ],
   "source": [
    "if available_models:\n",
    "    model_to_use = available_models[0]  # Use first available model\n",
    "    print(f\"Using model: {model_to_use}\")\n",
    "    result = simple_chat_example(model_to_use)\n",
    "    print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36a3f7-e89b-4f71-b05a-b8832f853eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
